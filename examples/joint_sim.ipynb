{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from hestia.similarity import *\n",
    "from hestia.partition import ccpart\n",
    "from hestia import HestiaGenerator\n",
    "from hestia.clustering import _connected_components_clustering\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/plumber.csv\", sep=',')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_field = 'seq'\n",
    "df2_field = 'smiles'\n",
    "\n",
    "sim_fun_ent1 = sequence_similarity_mmseqs\n",
    "sim_fun_ent2 = molecular_similarity\n",
    "\n",
    "sim_args_ent1 = {\n",
    "    \"field_name\": \"seq\",\n",
    "    \"threshold\": 0.3,\n",
    "    \"verbose\": 3\n",
    "}\n",
    "sim_args_ent2={\n",
    "    \"field_name\": \"smiles\",\n",
    "    \"fingerprint\": \"ecfp\",\n",
    "    \"radius\": 2,\n",
    "    \"threshold\": 0.3,\n",
    "    \"verbose\": 3,\n",
    "    \"bits\": 1024\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_df1 = df.drop_duplicates(df1_field).reset_index(drop=True)\n",
    "df1_to_df = df.groupby(df1_field).apply(lambda g: g.index.to_numpy())\n",
    "\n",
    "sim_df_1 = sim_fun_ent1(df_query=unique_df1, **sim_args_ent1)\n",
    "train, test, clusters = ccpart(\n",
    "    df=unique_df1, sim_df=sim_df_1, threshold=0.3, verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices, test_indices = [], []\n",
    "for indx in test:\n",
    "    indcs = df1_to_df[indx]\n",
    "    test_indices.extend(indcs)\n",
    "for indx in train:\n",
    "    indcs = df1_to_df[indx]\n",
    "    train_indices.extend(indcs)\n",
    "\n",
    "test_df, train_df = df.iloc[test_indices].reset_index(), df.iloc[train_indices].reset_index()\n",
    "\n",
    "u_test = test_df.drop_duplicates(df2_field).reset_index()\n",
    "u_train = train_df.drop_duplicates(df2_field).reset_index()\n",
    "\n",
    "u_test_to_df = test_df.groupby(df2_field).apply(lambda g: g.index.to_numpy())\n",
    "# u_train_to_df = u_train.groupby(df2_field).apply(lambda g: g.index.to_numpy())\n",
    "\n",
    "unique_test_mols, unique_train_mols = u_test.smiles, u_train.smiles\n",
    "print(len(unique_test_mols)/1e6, len(unique_train_mols)/1e6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from rdkit import Chem\n",
    "    from rdkit.Chem import rdFingerprintGenerator, rdMolDescriptors\n",
    "    from rdkit.DataStructs import (\n",
    "        BulkTanimotoSimilarity, BulkDiceSimilarity,\n",
    "        BulkSokalSimilarity, BulkRogotGoldbergSimilarity,\n",
    "        BulkCosineSimilarity)\n",
    "    from rdkit import RDLogger\n",
    "    from rdkit import rdBase\n",
    "\n",
    "    def disable_rdkit_log():\n",
    "        \"\"\"Disable all rdkit logs.\"\"\"\n",
    "        for log_level in RDLogger._levels:\n",
    "            rdBase.DisableLog(log_level)\n",
    "\n",
    "    disable_rdkit_log()\n",
    "\n",
    "except ModuleNotFoundError:\n",
    "    raise ImportError(\"This function requires RDKit to be installed.\")\n",
    "\n",
    "radius, bits = 2, 1024\n",
    "fpgen = rdFingerprintGenerator.GetMorganGenerator(\n",
    "    radius=radius, fpSize=bits\n",
    ")\n",
    "sim_function = 'tanimoto'\n",
    "\n",
    "def _get_fp(smile: str):\n",
    "    mol = Chem.MolFromSmiles(smile, sanitize=True)\n",
    "\n",
    "    if mol is None:\n",
    "        print(f\"SMILES: `{smile}` could not be processed. Will be substituted by `{smile[1:-1]}`\")\n",
    "        return _get_fp(smile[1:-1])\n",
    "\n",
    "    fp = fpgen.GetFingerprint(mol)\n",
    "    return fp\n",
    "\n",
    "def _parallel_fps(mols: List[str], mssg: str) -> list:\n",
    "    fps = []\n",
    "    jobs = []\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        for mol in mols:\n",
    "            job = executor.submit(_get_fp, mol)\n",
    "            jobs.append(job)\n",
    "        pbar = tqdm(jobs, desc=mssg, unit_scale=True,\n",
    "                    mininterval=0.5, maxinterval=2)\n",
    "        for job in pbar:\n",
    "            if job.exception() is not None:\n",
    "                raise RuntimeError(job.exception())\n",
    "            result = job.result()\n",
    "            fps.append(result)\n",
    "\n",
    "    pbar.close()\n",
    "    return fps\n",
    "\n",
    "test_fps = _parallel_fps(unique_test_mols, \"Test mols\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "from rdkit.DataStructs import ConvertToNumpyArray\n",
    "def batched(iterable, n, *, strict=False):\n",
    "    # batched('ABCDEFG', 3) â†’ ABC DEF G\n",
    "    if n < 1:\n",
    "        raise ValueError('n must be at least one')\n",
    "    iterator = iter(iterable)\n",
    "    while batch := tuple(islice(iterator, n)):\n",
    "        if strict and len(batch) != n:\n",
    "            raise ValueError('batched(): incomplete batch')\n",
    "        yield batch\n",
    "\n",
    "def compare(mol, test_fps):\n",
    "    fp = _get_fp(mol)\n",
    "    sim = BulkTanimotoSimilarity(fp, test_fps)\n",
    "    return sim\n",
    "\n",
    "all_results = set()\n",
    "threshold = sim_args_ent2['threshold']\n",
    "\n",
    "test_size = len(unique_test_mols)\n",
    "pbar = tqdm(unique_train_mols, unit_scale=True)\n",
    "import copy\n",
    "tmp_fps = copy.deepcopy(test_fps)\n",
    "prev_len = len(tmp_fps)\n",
    "tmp_u_test = copy.deepcopy(unique_test_mols)\n",
    "\n",
    "for idx, mol in enumerate(pbar):\n",
    "    fp = _get_fp(mol)\n",
    "    out = BulkTanimotoSimilarity(fp, tmp_fps)\n",
    "    out = np.array(out)\n",
    "    removed = [f for mni_idx, f in enumerate(tmp_u_test) if out[mni_idx] >= threshold]\n",
    "    tmp_fps = [f for mni_idx, f in enumerate(tmp_fps) if out[mni_idx] < threshold]\n",
    "    tmp_u_test = [f for mni_idx, f in enumerate(tmp_u_test) if out[mni_idx] < threshold]\n",
    "\n",
    "    if len(tmp_fps) < prev_len:\n",
    "        out_u = np.argwhere(out > threshold)\n",
    "        all_results.update(removed)\n",
    "    if idx % 100 == 0:\n",
    "        pbar.set_description(f\"Include: {len(tmp_fps):,} / {test_size:,}\")\n",
    "        if len(tmp_fps) == 0:\n",
    "            print(\"Saturation\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loose_test = test_df[test_df.smiles.isin(all_results)]\n",
    "strict_test = test_df[~test_df.smiles.isin(all_results)]\n",
    "print(len(loose_test) / len(test_df), len(strict_test)/len(test_df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hestia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
